[[{"i":"#","p":["enjoy the hallways"]},{"l":"intro","p":["Glad to see ya Thanks for checking out the hallway. You can think of this place as a dumping grounds for everything in my brain. I've learned that while I'm a great learner, I have terrible retention of knowledge. If I don't write something down, it's lost to time one way or another. You'll find various areas of my brain on the left-hand side."]}],[{"i":"#","p":["enjoy the hallways"]},{"l":"start ups"},{"i":"#","p":["I often think about my sixty year-old self. What will I look back on and regret doing? Not doing? Did I sail the world? Did I really give up everything, all the momentum in my career, friendships, mortgages, and go get blasted with salt for years on end? Did I learn I get violently seasick and quit after a month? Or did I never take a big jump that I can be proud of?","For the first time, I'm confident at this moment 60 year old me is proudly looking back (time, after all, is a big ball of wibbly wobbly... time-y wimey... stuff). I quit my job! A job I had a lot of love for, with people that were incredible, that made money I was quite proud of. Why did I do it? Mostly manic anxiety about not living up to my potential, but also because I knew I needed to take that big jump in my life before it was too late. Something to hang my hat on once my knees finally gave out and my hairline ran away.","Even more concerning to some people, I did so with no idea about what I wanted to build, no revenue, and no customers. Exciting, isn't it? I think so too.","I kind of hate the current hustle culture of audience building, twitter slams, and whatnot, but I do still love writing. I've always enjoyed being able to express myself in a format outside of my head, so this isn't my attempt at building a substack that pulls in $5/mo, but to document my attempts at building something I love, and sharing some of the cool learnings along the way. Grammar be damned, I'll at least get some thoughts out there.","My small home away from home","To begin, I've leased a 60 ft 2(5 1/2 m 2) windowless co-working space to trap myself in 8 hours a day and try some cool shit. See ya soon."]}],[{"i":"#","p":["for when things get a little misty"]},{"i":"cloud-security-or-mistysec-1-0","l":"☁️ Cloud Security, or, mistysec 1.0"},{"l":"WTF am I doing","p":["When I resigned at my last job, I offered to stay on for six weeks to help ensure a smooth transition, since I was fortunate enough to have done a lot in my time there. During those six weeks, in between knowledge transfer sessions, I found myself stirring, trying to figure out what I wanted to build.","What were the problems I had identified in the world, that matched closely with my skill sets, and would also make me enough money to retire? It turns out basically nothing, or at least that's what it felt like. So rather than being original, I'd do what artists do best, imitate.","Cloud security has been exploding the last few years, wiz.io being the golden boy amongst us. Startup wisdom tells you that selling a shinier widget doesn't get you far, unless you love burning cash, and that you should have a differentiator, or something unique about you. A quick look at the top providers tells you that you aren't special, and there's not much that you can add that they don't already do. What does that leave?","My mind immediately goes to pricing, but that usually ends up in a race to the bottom. So if pricing is out, how else could I differentiate in the cloud security space? After looking around, I realized that every single CSPM covered exclusively:","AWS","Azure","GCP","IBM Cloud","Alibaba Cloud","So those platforms are all covered, what does that leave?"]},{"l":"Fight for the little guys","p":["I spend a lot of my time playing around with emerging technology platforms like Render, Supabase, and Vercel. Like most new technologies, enterprise security tooling is not the first area of focus for them. They get their SOC 2 Type II and move on. But what are they providing for their users to ensure they are utilizing all of the configurations properly? For example, Render's default settings expose your database to the entire world. How many people do you think notice that?","So that's it then, be wiz.io for the little guys. Offer enterprise security tooling downstream to the platforms that don't have enough users to attract the big players yet. How do I begin? To avoid the trope of a technical guy building a product no one wants, user research seemed like the most reasonable place to start."]},{"l":"Call me the NSA","p":["One of the things that I have going for me is I use to be a hacker (I'd argue I still am, but would never win a CTF at this point). I want to find users of Render and Supabase and ask them about their cybersecurity needs. In cybersecurity, we call this Open Source Intelligence (OSINT) and luckily for me, man is there a lot of it for this use case. For someone to use Render or Supabase they have to update their DNS records and point their domains to ones owned by the cloud provider. I could go around scraping and abusing DNS all around the world, or I could just use our friends over at host.io. You can look up a domain name or Autonomous System Number(ASN) and see all domain names that point to it.","After buying $500 worth of API tokens, I was able to start pulling all of these domain names down and prioritize who to reach out to. There were 27k domains, so how can I identify legitimate companies and not small blogging websites? In an ideal world, I'd use zoominfo but they don't really service companies of my size. Luckily for me, Hubspot exists. I was able to sign up for their Starter Suite for only $240/year and get access to a pretty sweet set of tools. From there I imported all the domain names and hubspot automatically correlated the domain names to businesses, providing me their sector, revenue, employee count, and more. Unfortunately a lot of this data was pretty hit and miss, it would end up correlating small businesses on the platform to major providers. I doubt your local community center is doing $1,000,000,000 in revenue. I spent a good few days parsing through this data in a variety of ways.","After finding an eligible business, I would look it up on LinkedIn Sales Navigator and reach out to the most senior technical leadership or founder with a message along the lines of:","Sales is a funky thing for me and I imagine most technical people. It can come off as pretty ingenuine and scammy. In most cases, it usually is, so that's a fair thought. Like most things though, I think if you're a good person, working with good people, everyone benefits. To me it felt weird sending hundreds of LI connections asking for advisors, but I routinely found that when I got on a call with someone they were kind and gracious and happy to help. I'm immensely grateful to the people that have taken me up on these offers in their various forms."]},{"l":"Text content does not match server-rendered HTML","p":["Like everyone's favorite React error, my thoughts and reality didn't exactly match up. Through my conversations I realized that the types of businesses that have $50k+ to spend on cybersecurity tooling are unfortunately not on Render or Supabase yet. At least not in any measurable capacity worth building a business around. I was pretty disappointed by this, I want to work with emerging companies that I find exciting and keep me engaged. But building a start up is all about this, hitting issues and finding solutions. And there's no one around to grade your homework, so you better be smart about it.","Side-note: this is a common thing you'll see in my writing and exploration. I've struggled to identify when a roadblock is a disqualifying event for a valid business, and when it's a normal challenge that you simply have to overcome to continue down the same path. In this case, I was pretty sure it was a disqualifying event."]},{"l":"Swimming up stream","p":["I realized I shot pretty far downstream from AWS to Supabase, so I look at who might be in the middle. It turns out there's a pretty mature middle market for cloud providers, comprised mainly of companies like OVHCloud, Vultr, and Hetzner. Wash, rinse, and repeat. I went to host.io (finally putting that $500 to work) and started pulling customers for these providers. Immediately, I'm motivated by seeing how much larger their client base is from the previous providers.","OVH Cloud has 7 million+ domains pointing to them","And that was another week, scraping, triaging the information, reaching out on LinkedIn, and talking with customers of those platforms. Again, I was surprised that even at this level, aside from some whales, there didn't seem to be a large client base that would be spending enterprise dollars on security. I found a lot of wordpress hosting providers, game server hosting companies, the odd tech company, but no one I met with said that security was in their budget. I'm sure the people are out there, but I couldn't find them. Even now I'm doubting myself and thinking I should go back and keep trying to find those companies, but moving fast is important and I felt this was a big enough barrier to warrant moving on."]},{"l":"Thanks for the fish","p":["That's kind of where I left it. I didn't want to build a new shiny AWS widget and the differentiation I thought I could build around didn't exactly work out. However, I'm glad that I took the time to do market research, talk with customers, and validate the ideas. I could've spent 3 months building the platform and been SOL."]},{"l":"For the nerds","p":["A little postscript here, I did take a few days in this research to look at some technical options for building this platform. I wanted to explore some new programming languages/frameworks and see if any are suitable for easily building enterprise CRUD apps. The one that stood out to me most was Laravel Filament. Ewwww, php, you're saying. I know, get over it. It was actually a really interesting set of tools that solve a shit load of problems. Filament is primarily built for internal tooling, not full SaaS apps but it can do quite a lot. At the end of the day though, I couldn't get over my issues with PHP (now you can say ewww). The hardcoding of strings to reference parameters all over the place, a barely existent type system, and just my general unfamiliarity with PHP made me ditch that route. Killer ecosystem though, check it out."]}],[{"i":"#","p":["bootstrapping my way to being non-bootstrapped"]},{"i":"business-models","l":"\uD83D\uDCBC Business models","p":["Ahh yes, business. The reason we all get to wear silly little clothes and get up early every day. A big part of starting my own company (TBD) was being able to create an environment that matched the way I believe a business should be run. One that fits the way I want to live my life in general. This is a surprisingly huge problem area that I don't think enough founders spend time to think about before building their start up. Everyone wants to shoot for the moon, make a billion dollars, but forget they have to live a life during all of that as well."]},{"l":"Bootstrapping","p":["My favorite Minneapolis resident, Rob Walling, is the forefather of bootstrapping companies. Building something in a slow, consistent, measurable way, investing your profits back into the company. This is in opposition to the venture model, which burns cash, and lives, in the hope to someday make a profitable business. Man I'd love to be filthy rich, but honestly I have a short attention span and building a billion dollar company seems like it'd get boring after awhile. Hiring, politics, hiring, politics, board meetings, etc...","Bootstrapping has and still is a leading contender for how I want to run a company, in the back of my mind venture is still a thought (hi VCs!), and taking capital isn't a bad thing. However, the endless pursuit of funding rounds, layoffs, and bullshit isn't really in my arena."]},{"l":"Get to the point","p":["I say all this to try and frame the context for a lot of my writing. You might think: \"dude just get funding and give it a shot\". Meh? I'll have to take a dive eventually, but jumping uneducated into a problem with other people's money is a path for someone else.","This means that as I'm evaluating ideas and disqualifying potential opportunities, the type of business I'd have to build to pursue those ideas is a large part of the process. Is the idea something I'd be able to build and scale alone to a place of profitability so I could hire a second person? And some day a third? If I wanted to take growth funding, when would I need it, and what would I have to give up to do so?","This document will likely be a living document as my ideas on this change and shift, so check the commit history to watch me rewrite my own history and cover up my tracks when I eventually need to take VC funding."]}],[{"i":"#","p":["building magic with atlassian forge"]},{"i":"vulnerability-management","l":"\uD83D\uDC1E Vulnerability Management","p":["In the spirit of identifying an idea with a viable Business Model, marketplaces are a great idea. They offer you an already gathered set of customers, some well defined boundaries, and a reputable platform to build on top of. That's why App Stores are such a hit, they help you stand out from the breadth of garbage on the broader internet.","In my previous role, I lead a vulnerability management product, a space that someday I'll write far too much about. A common issue in the market was no matter what vulnerability management tool they used, it all went to Jira or ServiceNow. Why do these companies have such a chokehold on companies? After abandoning my cloud security research, I tried to find something that could build off of my experience, while not violating my non-compete. I figured we'll cross that bridge when we get there if the idea actually pans out."]},{"l":"Atlassian Forge","p":["Atlassian has a surprisingly fantastic framework for building marketplace apps called Forge. They run all the infrastructure, handle all billing and licensing, and charge you next to nothing ( likely to change next year). That fits very well with a bootstrapped model, removes a ton of overhead from me, and lets me focus on building and selling something meaningful. I'm still super interested in this space and might pursue an idea here.","Looking across the marketplace I don't see anything for vulnerability management but do see other security tools with good traction, so it seems like a good fit. Let's gauge the market."]},{"l":"Ruh roh","p":["So here we go again, Sales Navigator and cold outreach. I sent out probably 200 LinkedIn connection invites and talked to ~ 10 Vulnerability Management leads at large companies that I knew would have budget for security tooling. Every. fucking. single. one. of. them. uses Service Now (SNOW). Not a single one uses Jira for vuln management. There's a wall bridging security and development, and it appears security is behind the iron curtain of SNOW. I've worked with SNOW in the past and have no interest in building an entire company around it. Luckily, again, instead of building an app on Forge for 3 months and not selling it at all, I felt confident enough to disqualify this idea. Like everything, I know there are people out there that use Jira for vulnerability management and pen testing and would benefit from this, but you've got to read the tea leaves for what they are at that point.","I'm sure I'll be back though, the market opportunity for bootstrapped businesses on Forge is huge, for now it's on to the next thing though."]}],[{"i":"#","p":["building a bank? how hard could it be."]},{"i":"bank","l":"\uD83E\uDD11 Bank","p":["After failing to follow a couple interesting bootstrapped cybersecurity ideas I thought: \"why not do something extremely ambitious. Take the funding. Solve world hunger. Takeover the world\". What better way to do that than starting a bank!","I mean, how hard could it be, what is actually required to start a bank? It turns out, a shit load. I knew it wouldn't be easy, but sometimes the intimidating things in life are stupid easy. In this case, it's intimidating because it's hard. To actually get a state (non-credit union) banking charter, you need a significant (8 figures of funding), basically an entire compliance department from day one, and a love for the rigamarole of banking regulations. Maybe I wasn't cut out for this one. Surely there has to be a technology solution here though, right?"]},{"l":"Optimus Prime","p":["As always, there was a technology solution. They're known as neobanks:","A neobank is a type of direct bank that operates exclusively using online banking without traditional physical branch networks that challenge traditional banks. https://en.wikipedia.org/wiki/Neobank","All those shitty ads you get for credit cards, debit cards, basically anything not made by Visa or Mastercard? Yeah, those are neobanks. They're actually backed by much larger reputable financial providers, and they serve almost exclusively as marketing departments. These neobanks are usually run on Banking-as-a-Service (BaaS) providers. The two I spent time looking into where Treasury Prime and Column. Both mouth-wateringly attractive companies. The marketing, their API documentation, the pitch, it's :chefs-kiss:. For a more in-depth look Contrary Research has a fantastic market guide for Fin Tech Infrastructure. These companies could make it far easier to start a \"bank\". However, the more you look at the revenue models for someone building on these platforms, especially someone not trying to take on a ton of investment, it's hard. You primarily make revenue on interchange fees, which is the 1% you get on every credit card transaction. This means it's a numbers game, you need to get millions of users to scrape by, which is a marketing game. Me and marketing don't do very well together, I'm much more well suited for B2B enterprise sales than I am marketing. Still fucking cool though, what a fun space."]},{"l":"Megatron","p":["Alright, so not a bank. How about a cell phone company? When leaving my past company I almost jumped off the roof due to how hard AT&T made it to transfer off their plan to Mint Mobile. Could I make some improvements there? Through some research, I found the concept of Mobile Virtual Network Operators, which is basically what Mint Mobile is. You buy surplus network capacity from one of the large telcos, and market it to your users and make $$ on the difference. God damnit, there's that marketing again. I'd have to get millions of users to make it work, with a primarily ad-driven acquisition channel. Maybe next time..."]},{"l":"Those were the only transformers I knew","p":["I've got it, an insurance company! At this point, I'm a seasoned bigtech builder, I know how to make a bank, a mobile phone company, I've got this research path down. Insurance as a Service, where the fuck are you. Qover I see you, stop running! Embedded insurance is what they call it and again is filled with companies with beautiful API designs, great marketing, and seemingly simple ways to get started. Out of the 3, this is likely the one I'd actually have the most success at, due to the need for Cyberinsurance. I'd previously worked with and sold products for cyberinsurers, so there was some reasonable overlap there. Companies like Coalition are crushing it in the cyberinsurance space and most of their cyber tooling I could build in my sleep. To be honest, I'm not sure why I stopped pursuing this one, I think you need the backing and confidence of some pretty big people to get these embedded insurance providers to work with you. If you want to sell multi-million cyber plans, they require you to have some assets to back them up if it explodes. And sadly they wouldn't take a 2017 Mazda as collateral."]},{"l":"Sorry world","p":["I guess we aren't solving world hunger today, but it was fun researching these goliath markets and seeing that everyone has beaten me to the cool ideas in them."]}],[{"i":"#","p":["why not?"]},{"i":"-cdn","l":"\uD83D\uDEDC CDN","p":["Not exactly a CDN, more of a WAAP, but anytime I say WAAP people starting singing Cardi B lyrics at me. WAAP stands for Web Application and API Protection. The common example would be Cloudflare, but more closely would be Imperva and Akamai. These platforms proxy all of your traffic and perform DDOS protection, bot mitigation, web firewall rule matching, API inventorying, and more. Similar in grandiose size to a bank, but far more in my wheelhouse. At this point I'm about 4 weeks into my start up journey and am getting a bit sick of cold outreach on LinkedIn, so I decided to grace myself with a little technical research. I knew there was a market for this, I wasn't quite sure what my differentiator would be, but I wanted to see if I could get the basic concepts built first to recharge my batteries a bit.","To give you an introduction to what I built, these two diagrams can be pretty helpful and are explained below."]},{"l":"Infrastructure Diagram"},{"l":"WAF Request Lifecycle"},{"l":"Cheap AF","p":["So an edge network that can intercept traffic across the globe, cache it, inspect it, or whatever it, and then forward it to the destination server. That means aside from compute, our largest cost is likely going to be network egress fees, and as that link shows, it's not cheap. If you're looking at using AWS, your egress costs are 9.2x what they'd be on something like Digital Ocean. From the good ol' days when I was building a cloud security company, I remembered Hetzner and OVHcloud, and their pricing seemed great for egress. After further inspection though they don't have a very large global presence. Hetzner is mostly central Europe and OVHcloud had a lot of locations but limited support for various features of their platform (like cheap computing plans) across those regions.","After some bouncing around, Digital Ocean (DO) seemed to be the best contender. I'd used it in the past, they have cute marketing, and had a good blend between global coverage and cheap egress fees."]},{"i":"can-you-hear-me-now","l":"Can you hear me now?","p":["The next hurdle was to try and build a network of servers around the world that could communicate with each other and proxy traffic. My brain immediately goes to Kubernetes (k8s). I use to manage a cluster in my last job, which thankfully we migrated off of, and am somewhat familiar with it. (Spoiler alert: he wasn't familiar enough with it). There are a lot of options for managing k8s:","you can raw dog it by yourself","use a managed service like EKS or GKE","use some of the enterprise managers like Rancher","There are also some fun options like k3s and its variants that give you a simplified method to install k8s. I took a brief look at Nomad but the ecosystem sadly isn't robust enough for me to feel comfortable adopting it, but they do have multi-region federation which is more than I can say for k8s, whose federation seems to be eternally in beta.","It turns out, choosing k8s despite not having multi-region federation was going to be a problem. Follow along with my stupidity that wasted 2 days. I decided not to use Digital Ocean's managed k8s because it only supports deploying the cluster in a single region (hmmm...I wonder why they do that...). So I opted to use Rancher Kubernetes Engine 2(RKE2) since I could control how to deploy the nodes. I spun up worker nodes in DO's New York City (NYC), Amsterdam (AMS), and Sydney (SYD) regions and deployed a simple http server into them."]},{"i":"how-about-now","l":"How about now?","p":["Cool, so I have a k8s cluster running across multiple regions, but now I need to figure out how to route traffic to different regions based on geographic location. You have 2 options when doing this, one is lease a /24 IPV4 range, bring it to a cloud provider, and broadcast it via BGP Anycast on nodes around the world (something DO doesn't support, btw...). You can see a good overview of Anycast here if you're interested. The other option is using GeoDNS which lets you serve different DNS records based on the location of the user, or their proximity to your servers. That seems way simpler, albeit less robust. I threw in some records and pointed them at my hosts."]},{"i":"now","l":"...now?","p":["Finally, I can try and see if they can hear me. I send a request on my test network to https://dev.mistysec.com and.................timeout. What the fuck, dude?! I actually don't know why it took me this long to get around to testing the web server, but I wanted to get over the DNS hurdle and didn't expect an issue here at all. I'll save you the day of debugging I did and cut to the end, basically k8s isn't built to have its nodes geographically dispersed. Even if I send an HTTP request directly to an ingress node in NYC, it will still sometimes communicate with the node in AMS to see if it can handle the request. And occasionally, it'll also route the request over there. They don't really make any promises, or give you any (easy) controls, for routing traffic based on geolocation. And the options they do give you don't remove the latency at the control plane."]},{"l":"Switching to Mint Mobile","p":["Well that sucks. One thing that has always bothered me about k8s is that it's got a ton going on inside of it, a lot I don't know about, and definitely don't know how to fix. So I tried to strip my requirements down to the bare minimum and see if I could build what I needed from scratch. I would need:","A service mesh, so the servers can communicate with each other. For example, to retrieve the SSL/TLS cert for a host to decrypt its traffic","The ability to push containers to edge nodes and routinely update them","A private network for this communication to happen across"]},{"i":"what-the-fuck-is-a-kilometer","l":"What the fuck is a kilometer?","p":["Okay, so, a service mesh? I could probably drop this phrase in a conversation in silicon valley and get away with it, but I really had no idea what it was. Still don't. But at my base understanding, it's a way for a group of servers to discover and communicate with each other. Istio is a popular example for how this is handled in k8s. I spent awhile researching, and wish I remembered all the weird things I looked at, but at the end of the day I ran into NATS which is a streaming and message passing platform (and also a lot more). Think Kafka, but sexier. This means I could have a NATS server in each region of my cluster, and then the edge nodes only need to connect to their local server. From there, they communicate with other servers via message subjects, e.g. acme.getCertificate, instead of needing to know their IP/Domain name. The servers then form a regional cluster and a global super-cluster and handle passing messages across the architecture. Long story short this means aside from the NATS servers, all my other servers only need to communicate with hosts in their region. And the NATS servers handle cross-region communication and private networking. Side note: they have a sweet piece of tech called nex coming out, that I wanted to use for my distributed containers here, but it was a bit too early in their alpha for my uses.","Alright, so all the servers can communicate, but it doesn't seem great to have them doing that over the public internet? NATS seems secure, but you really shouldn't be publicly exposing these ports.","Now, I know, at this point you're saying: \"Jake who fucking cares if it's secure? You have no customers\". And to that I would say...touché"]},{"l":"En Garde","p":["In some cases, your service mesh will also handle securing your communication, and honestly one could argue using NATS with TLS would be secure enough as well, but what if I also want to do non-NATS things on this network? I looked at a couple commercial options, like Tailscale, but didn't find any that you could deploy entirely yourself. I didn't want to use Digital Ocean VPCs because if this thing grows I'll likely move it to bare metal hosting with another provider and don't want to rewrite the networking. I ended up going with Wireguard for a couple reasons:","It supports peer-to-peer connections, so I don't need a massive global network","There is a user-space golang library(I wouldn't know this until later...)","Fully open-source and easy to deploy on-prem"]},{"l":"A brief aside","p":["Something I haven't mentioned yet is how I'm handling all this infrastructure. In simple terms, I'm using Pulumi with Remote Commands as a pseudo-ansible to handle any resource provisioning after the VM starts up (e.g. deploying a new docker container, updating wireguard routes)."]},{"l":"Back at it","p":["So I updated Pulumi to install wireguard with the base cloud-init configuration for all my servers, and added remote commands to add new peers as they are spun up. This is sick! My grand plan is all coming together. Now all I have to do is take my central control server, which runs next to my web server, and connect it to wireguard so it can send commands to the NATS cluster. One part I haven't mentioned is that I have a web application for typical user-facing stuff and a control server, which handles communications between the web server and the edge network. This is hosted on Digital Ocean as well, using their App Platform. I updated the code for the control server to add wireguard's userspace library and deploy it. err: Unable to update bind. \uD83E\uDD2C. After another few hours of exploring it seems that most container platforms don't allow you to do a lot with the network settings of the container, even though this one should be entirely user space. You typically need to add something like --cap-add=NET_ADMIN to your docker run command to make it work. Annoyingly, even if I stripped my local docker containers of all permissions I couldn't replicate the same error, which made debugging hard. I looked at a lot of workarounds and solutions and could not find anything that would work.","I tried deploying the same docker container on Render and got the same issue. Luckily though, they also support Native Runtimes which for some reason don't have the same issues with network capabilities. I had already had some other annoying experiences with DO's App Platform at this point, so I moved over to Render and everything was happy."]},{"l":"Time to spy","p":["I updated my simple web UI to allow a user to provide a domain name and a host they want to proxy it to, it would then verify the DNS points to our GeoDNS location and start proxying. When a request comes into an Edge Agent, it would send a NATS request to the control server asking for the proxy settings of that host, and would then use those settings to route the request accordingly. But the internet doesn't run on port 80, so we have to deal with TLS certificates. Let's encrypt is the popular option here, so I began looking into how to programmatically work with it. I looked at some libraries for this, but none gave me the control and flexibility I wanted, So I decided to implement my own lightweight ACME client. I followed this awesome guide, but it was written in Ruby and I was currently using typescript in the control server. A little ChatGPT later and I have that guide converted to typescript. The timelines here aren't exactly chronological, I actually did this before I ran into the wireguard issues outlined above. So when I ran into those issues I migrated the control server from typescript to golang, so I could use the userspace wireguard library. With that, I used ChatGPT to convert my typescript ACME client to golang. A bastardization that should never see the light of day.","After some tinkering, I was able to get the certificates provisioned and communicating through NATS. Now when a request came into the Edge Agent (Go Gin web server) it would intercept the SSL/TLS handshake, get the SNI from the Client Hello message, reach out via NATS to the control server and ask for the TLS certificate, and return it to the requesting client. After the handshake completed, the agent would ask for the proxy configuration, and forward the requests along to their destination host."]},{"i":"ready-aim-firewall","l":"Ready, aim, firewall!","p":["Everything's hunky dory, intercepting and proxying TLS requests across the globe, but what's a WAAP without a wet ass...I mean a web app firewall. I originally thought about throwing out NGINX with ModSecurity CRS on it and calling it a day, but as always I wasn't able to achieve the customization I wanted out of it. Luckily for me, I found Coraza which is a rewrite of Modsecurity in golang. More luckily, it's also usable as a library in go. Plugged that into our web server, messed around with some configurations, and we were now globally scrubbing malicious requests before they ever reach our destination server.","That wasn't so hard...was it? It only took two weeks of insanity and tearing my hair out, but hey, we got there."]},{"l":"Uff da","p":["Yeah that was a lot. Honestly I'd open source the code, but it's mostly garbage. If you're interested in looking at it, shoot me an email at me@jakereynolds.co. I went on to do some user research after this and when talking with users, it seems that WAFs are mostly commoditized and no one wants to proxy their prod traffic through a network that isn't cloudflare scale. I kind of had that expectation going into this, but I think there's a lot of components from this research that I'll be able to reuse in other projects."]},{"i":"thanks-for-reading","l":"Thanks for reading!"}],[{"i":"#","p":["just put the 5g in me already, bill"]},{"i":"cookie","l":"\uD83C\uDF6A Cookie","p":["Third-party cookies are starting to be banned in various browsers. This is a good thing because ads and the loss of privacy on the internet suck, but it also introduces new issues for cybersecurity. Primarily, being able to verify the authenticity of various users across websites."]},{"l":"Please complete the captcha","p":["Thanks DALL-E Whenever Cloudflare makes you adopt a puppy to prove you're human it tags you with a cookie so everyone knows you're one of the good ones and they don't have to spam check you again for awhile. With the loss of these cookies that mechanism is becoming harder and harder to use. To solve this, various internet giants came up with the Privacy Pass protocol for secure, anonymous token passing. With this protocol people like Cloudflare can now verify you're a human and issue you a set of tokens, that other websites can request. These tokens are fully(sorta) anonymous and websites can request them to verify you're not a robot, without disclosing who you are. I say sorta because Mozilla has some issues with Privacy Pass and outlines them explicitly in their research paper.","Issues aside, it's an emerging technology that has been adopted by huge providers, but has limited tooling available for it outside of those providers. You'll see various implementations of the protocol like Apple's Private Access Tokens or Google's Private State Tokens. These implementations typically provide both major parts of the protocol by themselves: the Attester and the Issuer. The attester validates that you are human and the issuer issues you a set of tokens that can be exchanged with other websites to prove you've been attested. Interestingly enough, you can submit to Apple to become an Issuer as long as you fit some criteria.","And Google simply requires you to open an issue on their Github repo."]},{"i":"apple-s-pat-issuer-requirements","l":"Apple's PAT issuer requirements"},{"l":"Show me the money","p":["This is all pretty fancy, but what sort of business models does this expose for us? Can you monetize being an issuer, or sell some sort of product that helps companies implement these tokens? Becoming an attester seems lucrative, as there's a limited market for mature human verification these days. Commonly just hcaptcha, turnstile, or recaptcha. But there's a reason that's a hard market, it requires a lot of training data and AI is bypassing the shit out of most of these providers. Cloudflare stopped providing captchas awhile ago, in favor of a more non-interactive mechanism. But you'll find many providers bypassing captchas as a service.","That's kind of where my research left it, I wasn't really sure there was a reasonable market of tooling to build around this ecosystem. Smarter people than I will figure something out, but for now I'm going to put this one on the shelf."]}],[{"i":"#","p":["breaking news, scraping the internet is hard"]},{"i":"msnbc-for-github","l":"\uD83D\uDCF0 msnbc for github","p":["I'm in love with technology. Almost pervertedly so. Not in a furry kind of way, but more of a getting married to a roller coaster kind of way. Few things in life excite me as much as technology. Finding new tools to play with, buying new hardware, writing code, solving problems, \uD83D\uDE0D. It's something I've been lucky to love in life and have the opportunity to do for a living. When thinking about start up ideas, tapping into this love is something that I commonly come back to. How can I build a platform that keeps me on the cutting edge of techology. Should I start a newsletter about emerging tech, create a youtube channel trying out all the latest frameworks, or provide training to enterprises to help them adopt new methodologies? So far nothing has stuck, but this is about one idea I pursued."]},{"l":"Morning routine","p":["My morning routine consists of many weird little things, like always having to put on deodorant before I brush my teeth. But I do that so it has time to dry before I have to put a shirt on. Anyways, once I'm clothed and in my office I check the front page of Hacker News, the Github Trending page, and unfortunately Twitter for a little bit. One thing I'm routinely disappointed by is that the Github Trending page only updates once a day and provides a max of 25 results. My insatiable thirst for more technology is always left unquenched. So I figured, why not try and build something around this?"]},{"i":"heres-the-pitch","l":"Here's the pitch","p":["A platform, a world wide web technology platform you could even say, that tracks all of the available code repositories on the internet (not just Github), aggregates statistics about them and reports them to you. This way you can get a far more in-depth trending page and more interestingly try and draw intelligence about the interactions between these repositories, to answer questions like:","What is the most common date picker library used by React developers?","How many apps have been rewritten from React to Vue?","What dirty little secrets are those C developers hiding?","There are various iterations like this out there that aggregate stargazers and commits and let you search through them, but none quite captured my vision."]},{"l":"How does it work","p":["So Github definitely doesn't like people using their API too heavily, they famously don't have a pay-for-use API plan, and have nice but non-suitable API limits for projects like this. That can be a future problem though, what's a small test case I can try out before we go whole hog on this? How about scraping all of the available package.json files from Github, so we can try and correlate dependency usage across projects. Their Code Search API should be good for this. If we use large page sizes and take our time, rate limits shouldn't be a problem. Some Typescript along the lines of:","and you can start iterating through all package.json files on Github, in descending order of popularity. Deeper in the code I parse out all of the dependencies and goodness but this was the hard part. Fun fact, getting files from raw.githubusercontent.com doesn't count against your rate limit, which is the reason for that substitution. Since I solved that, I should go ahead and build everything else! I went through building out some dependency parsing logic, stuffing it into a DB and displaying it in a UI. Only for me to discover, 2 days later, this note in their docs","the GitHub REST API provides up to 1,000 results for each search","which I didn't think would be a problem because my search showed millions of results in the total count parameter. Turns out, if you try to paginate pass 1k results, it throws an error...So we're a bit fucked on that one."]},{"i":"eminem-no-npm","l":"Eminem? No NPM","p":["Despite my issues, I know there's a lot of large websites out there that aggregate results from Github, how do they do it? Likely enterprise agreements, but maybe there's hope. Since we're working with the javascript ecosystem right now, even though future plans include multiple languages, let's try NPM. Maybe they have a nicer API.","As it turns out, they are more liberal with a lot of their data, and if you look hard enough you can find their API documentation in a random GitHub repo. Even more weirdly, is a file in that repo called follower.md. This details a mechanism for creating a NPM registry follower, that will sync all data in their entire database to a remote follower that you own. Holy shit! I've been a good boy, but I don't deserve such luxury.","This follower will sync NPM's CouchDB replica at https://replicate.npmjs.com and stream it from the beginning of time. A lot of this went over my head, as the data it streamed contained various events across the DB, all sorts of different fields, and a lack of documentation for my small brain. Nevertheless, it was pretty interesting to see, I was able to get a lot of the information I wanted, primarily dependencies, and even get them across versions. So I solved this for the Javascript ecosystem at least, but surprisingly life does exist outside of JS."]},{"l":"Thanks for all the fish","p":["Sadly the issues with Github's API were a big issue and would prevent me from expanding a lot of this research to other languages. I could try going the same route with other package managers for the other languages, but I lost steam and decided to move on to some other ideas. How's that for anticlimactic? \uD83E\uDEE1"]}],[{"i":"#","p":["greek goddess or primordial planet?"]},{"i":"theia","l":"\uD83D\uDC40 theia","p":["The Greek Goddess of Sight, or alternatively, the primordial planet that smashed into Earth to create the Moon. Theia was an idea I had to build out a sensor network of honeypots to be able to report on emerging trends in internet exploitation. You can think of it as being very similar to Greynoise."]},{"l":"Please help me","p":["My assumption was that ChatGPT would be a massive help here. I wanted to look at a lot of non-http protocols, things like SMB, SMTP, etc... and see what people were throwing at them. Given I know next to nothing about SMB, I figured I could convince Jippity to write a handler for me. As expected, when asking it to write a golang server that'll accept SMB, I get:","Implementing the full Server Message Block (SMB) protocol handshake in a simple Go program is a complex and extensive task due to the SMB protocol's complexity and the security implications of handling such connections.","and then it spits out a bunch of golang functions with // Implement parsing logic here. comments above each of them.","Before we get ahead of ourselves though, let's try to just get SSH working.","That was pretty easy, spin up a server on 22, enable password authentication, and we can now start logging the dictionaries people use to try and bruteforce SSH passwords! A little fun fact for the attentive reader, I'm writing this about 4 months after I shut down the project so I don't have access to the data anymore sadly. But the password results were basically what you'd expect, lots of admin:admin and 1234:1234 were submitted. I then stored all of these analytics in a Clickhouse database, because I expected to scale quite significantly and only needed to perform OLAP queries on it.","I was then quickly able to add the same functionality for the following:"]},{"l":"SMTP login detection"},{"l":"Port scanning detection"},{"l":"HTTP requests"},{"l":"Data feeds","p":["I also wanted to add some data feeds so I could provide IP address information, which came from the lovely ipinfo.io. The other piece of data I thought was cool was aggregating all TOR exit nodes, and being able to see if any of our traffic was coming from them. Easily enough, that's available here. That was quick! I did end up getting a few hits from TOR exit nodes, primarily for web traffic if I'm remembering correctly, so that was pretty nifty."]},{"l":"Setting Sail","p":["I deployed this on AWS across ~ 15 regions and let it run for 3 months or so. Unfortunately work got busy and I didn't have time to invest into the project and decided to shut it off to save my cloud computing bill :/. I still think this would be an awesome project, however the people over at Greynoise are doing a killer job, so it'd be extremely tough competition.","As always, another anticlimactic ending for ya, but theia was too cool of a name not to write about."]}],[{"i":"#","p":["how many licks does it take to get to the center of a data center?"]},{"i":"data-center","l":"\uD83C\uDFD9️ Data Center","p":["This is a brief aside because I found some cool technology and wanted to remember it. This wasn't really as much about building a physical data center as it was about managing a virtual data center. In relation to my research around CDNs I was thinking about how I would deal with managing physical servers, and maybe even letting people run compute on them. While looking at solutions, I realized there's a whole market of data center management software. It's pretty obvious when you think about it, but whatever. Some of those offerings are:","https://www.openstack.org/","https://harvesterhci.io/","https://maas.io/","https://www.proxmox.com/en/","All of those solutions solve slightly different problems and have different licensing/pricing models. Openstack is what got me really excited, but after reading reviews about it the general consensus seems to be it's a nightmare to maintain and the documentation they have available is miserable.","Harvester looks really cool, but it actually runs on top of kubernetes, which is a nightmare I'm not ready to repeat. Proxmox is pretty popular and many people use it for managing homelabs, but it's closed source and pretty expensive.","MAAS was the last provider I looked at and probably the most likely one I would use if I decided to pursue this path. I wasn't able to find too much online discourse about it, but canonical is reputable, their docs seemed good and they have a shit load of functionality. I played around with the idea of grabbing some bare metal and trying it out. I was already familiar with some bare metal hosting providers and Equinix was my favorite. However when looking at pricing, their bare metal hosting is more expensive (?!) than comparable Droplets at Digital Ocean. I always thought we were paying extra for using a VPS, but unless you physical own the bare metal you're running on it seems to even out in this case. So I'll put a pin in this unless I can find some far cheaper bare metal."]}],[{"i":"#","p":["the government is always watching"]},{"i":"snitch","l":"\uD83E\uDD2B Snitch","p":["A recent SEC ruling now requires companies to file an 8-K form declaring they have experienced a data breach within 4 business days of discovering it happened. This is huge and likely a big PITA for a lot of companies. Researching what happened and properly communicating can take a lot of time, but unfortunately a lot of companies abused this to try and sweep the breach under the rug. The SEC is having none of it. I had the idea that it'd be nice to build a notification service so you can be notified every time an Item 1.05 8-K filing is made."]},{"l":"Thanks Obama","p":["Luckily it's the government, and in some(ish) cases their data is public. You can search all public SEC filings via EDGAR, the Electronic Data Gathering, Analysis, and Retrieval system. You can go even deeper by using their full-text search to dive into all the filings.","Unfortunately an 8-K is a common form used to notify investors and the SEC about a variety of things, only one of those items being a data breach. Searching EDGAR for 8-K filings returns a ton of documents. Mostly about changes in executive staffing or compensation. But thanks to their, likely lucene backend, we can add a full text match for the words \"Item 1.05\" and get more specific results. At the time of writing this, there are 5 results across 3 companies. When I originally performed this research there was only 1 filing. Because of that, I wasn't sure how often new filings would come out, and how they would differ in formats so it wasn't an appropriate time to start writing automation around it. However as more filings get created and we can start to measure their velocity, this could be a fun data feed or subscription service to provide those that like others' dirty cyber laundry."]}],[{"i":"#","p":["notes on the foundational aspects of neural networks"]},{"l":"neural networks","p":["This is my working area for my current machine learning research. I'm starting back at the basics to try and rebuild my foundational knowledge. Most of this is my note taking while following Andrej Karpathy's Neural Networks: Zero to Hero course on Youtube. Fun fact: did you know he was badmephisto?!?! I used his cubing tutorials a decade ago, insane.","This section is a heavy work in progress and contains many factual inaccuracies. Additionally, it's a summation of various materials I've used to teach myself and they may not all be attributed. Nothing in here is a novel idea and was usually taken from somewhere else. I try to credit sources where appropriate."]}],[{"i":"#","p":["how neurons work in neural networks"]},{"l":"neurons","p":["Neurons are exactly what they sound like, the things in our brain!","In machine learning, we model these in neural networks to simulate how the brain works. Neurons take in a series of values (x) and weights (w), which are individually multiplied and then added. The neuron fires by taking these and adding the bias of the neuron (how trigger happy it is) and passing it through an activation function which helps squash the values to something like -1 to 1. This is usually tanh or a sigmoid function."]},{"l":"Weights","p":["The weights for each input of a neuron are arbitrarily chosen. There's probably a whole field of mathematics that goes into determining the best starting weights, but at this point for me, it's random. Then through the process of training, these weights get adjusted to try and fit our loss function."]},{"l":"Logits","p":["Usually when starting you'll choose random numbers as weights, these numbers can be either positive or negative. Because of this, we'll commonly get the exponent of the values by doing e^x. Then all negative numbers will be [0,1] and all positive numbers will be (1,inf]. These are commonly referred to as Logits.","Logits are the outputs of a neural network before the activation function is applied. They are the unnormalized probabilities of the item belonging to a certain class.","In the case of our bi-gram NN model the logits represent for each input character, the probabilities of each output character in the set."]},{"l":"Biases","p":["Much like weights, biases are also randomly chosen and updated throughout the training process to try and adjust the activation of that neuron to fit our loss function."]},{"l":"Random math","p":["It can be useful to plot the values of your activation functions and gradients. You ideally want to see a gaussian distribution when doing so because that means your network doesn't have a ton of dead neurons."]},{"l":"Layers","p":["Idk where to put this, but an interesting pattern was covered, which is during training each layer is commonly composed of 3 parts. A convolutional/weight part, a normalization part, and a non-linearity part."]}],[{"l":"activation functions","p":["Activation functions are called on the dot product of the weights and their inputs plus the bias of a neuron. This function helps us control the types of outputs we want to come from our neuron.","When using activation functions its important to look at how the weights of that neuron affect the activation function. For example if you're using tanh which squashes your values to [-1,1] and you have a lot of values >10 or <10 you'll have a lot of values that are -1 and 1 which basically makes the neuron useless. You can visualize this in PyTorch quite nicely as well. Things like normalization and standardization come in handy here.","Some further reading is available here.","The reason we need activation functions (which are also called non-linearities) is because if we only have linear layers, no matter how many we add, the all collapse into a single linear equation. Which means the network is basically useless. We add non-linearities to the network so that each layer adds a form of intelligence when combined with the other layers."]},{"l":"Softmax","p":["A softmax activation function is a mathematical expression that will take the outputs from a layer or a neural network and distributed them into probabilities, such that all the outputs sum to equal 1. This is an easy way to add probability distribution to the output of your net."]}],[{"i":"#","p":["normalization of neuron weights and biases"]},{"l":"normalization","p":["It's important to look at how you initialize the values for your neurons. You essentially want them to be gaussian at the beginning so everything has a fair chance of training. If you look at the following, already after one layer, our standard deviation has almost tripled after performing the dot product.","To fix this, the referenced paper above ended up in the addition of the kaiming_normal operation, which divides the weights by the square root of the number of inputs to each neuron. It's a bit different depending on exactly which non-linearity you're using. For simplicity though you can commonly divide your weights by the square root of the fan-in, or the number of inputs to your neuron.","Multiplying your weights by this changes the standard deviation because if you look at a layer with standard deviation 1 and multiply it by .2 the standard deviation is now .2.","So following that, we can set the standard deviation of a gaussian distribution by multiplying it by whatever our ideal distribution is. The kaiming documentation gives us the equation","where gain is precalculated by them in the docs. For tanh it is \\frac{5}{3}. And fan_mode is the number of axons of the neuron. Applying this to our function","So if doing it manually we would want to multiply our initial weights of a neuron with 10 axons by .3084 to have a more normal standard deviation after passing through the activation function. This helps with tanh because it's a squashing function, so we want to amplify the values a bit, to fight back against it squashing everything inward. This normalization is typically fixed by applying something like batch normalization to each layer, reducing the important of the initial weights values."]},{"l":"Batch normalization","p":["A concept published in this paper, provides a mechanism so that you don't have to normalize your weights at initialization but instead normalize them through the training cycle.","It's pretty complex and I don't fully understand it, but one critique of it is since it's working with the mean and standard deviation of each layer, it's coupling all of the inputs during training. So then independent inputs now have a dependency on each other and can affect training. What does that mean? I have no clue.","Another weird trait is that due to the way you calculate batch normalization, the biases actually ended up being effectively zeroed out. So when doing batch normalization you don't use bias in your neurons. It's important to remember the bias values wouldn't actually be zero, but because you're adding a constant value, the subtraction of the mean in the normalization removes any effect that bias would have.","You apply batch normalization on any layers of your choosing before or after the activation functions have been calculated, it's most common to do before though. You generate the normalization with the following"]},{"l":"Inputs","p":["It's also common to preform normalization and standardization of your inputs, so that they follow a more equal distribution as well. A good analogy is miles driven, if some users drove 100 miles and others drove 100,000 that creates a significantly large data set that could have large imapcts on the gradients of the neural network. We can normalize them by squashing them down to 0-1 and standardize them by the equation in Batch Normalization","This is discussed in this video. Normalization and standardization seem to be used somewhat interchangeably."]}],[{"i":"#","p":["different types of neural networks"]},{"l":"types of neural networks","p":["Introductory explanations of the different types of model types that exist. Notes that apply to multiple types will commonly end up in this document. Models isn't necessarily the best word for everything in here, but I'll take it."]},{"l":"Start and stop tokens","p":["You'll typically create a special start and end character. Examples being something like S and S and this helps your model know when you're beginning a sequence or ending a sequence. However, you always know when you have or have not started processing a word, which means you don't need unique characters for start and stop, you can use the same character. Based on where you are in your processing you can contextually know whether it signifies starting or stopping."]},{"l":"Model smoothing","p":["This is the process of replacing zeros in your model with ones so that you don't ever run into cases where there is 0% chance of something resulting in a loss of infinity. There are likely more uses of this term, but so far this is the only one I've encountered."]}],[{"i":"#","p":["bi-gram language model"]},{"l":"bi-gram character-level language model","p":["While not exactly a neural network, it's a good example to build off of and to reference as we build multi-layer perceptrons. These models are able to predict the next character given a sequence of 1 or more previous characters. These are as opposed to word-level language models."]},{"l":"Bi-gram","p":["Predicts the next character in the sequence exclusively based on the current character. We do this by taking all of the character pairs in every word in the data set and calculating how often the first character is succeeded by the second character. We can store this in a 2d array with the rows being the first character and the columns being the second. We'll also add an extra row for the start and stop token.","We generate statistics for each character pairing, then sample from those statistics to create terrible names."]},{"l":"Code example"},{"l":"Bigram visualization"},{"l":"Weights and biases","p":["Since this isn't a neural network, there aren't weights and biases. However, there are still parameters. In this case, the likelihood of each character pairing is considered a parameter."]}],[{"i":"#","p":["multi-layer perceptron neural networks"]},{"l":"multi-layer perceptron","p":["Multi-layer perceptrons are the beginnings of a neural network. They consist of multiple layers of neurons, normalization layers, and more to create a neural network. It's important to have a solid understanding of what a neuron is before diving in here.","Since I'm very code inclined, here's the python that implements the following image:","The following code also uses the Value class from Back Propagation"]},{"l":"single-layer perceptron","p":["If we take the bi-gram example we can easily expand off of it to create a neural network with one layer of inputs and one layer of outputs and get roughly the same result.","And then we can sample from this simple neural network with the following"]},{"l":"Code example"},{"l":"Code example"},{"i":"xenc--w","l":"xenc @ W?!","p":["It took me awhile to get my head around exactly what was happening when you do logits = xenc @ W. When we did this with the bigram earlier, we could just get the probabilities for each input by calling prob[char]. We can actually do that as well here if we really felt like it with logits = W[ix][None, :], however as the math gets more complicated it's easier to stick to the matrix multiplication abstraction of that, which is xenc @ W. The [None, :] part takes care of converting the 27-length tensor to a 1x27 tensor."]},{"l":"multi-layer perceptron","p":["A multi-layer perceptron builds off of the single-layer perceptron concept, but adds in the concepts of activation functions, normalization, and more. The hardest thing for me to grasp in this is the embedding space. In the following code, you'll see we have a hyper parameter called EMBEDDING_SPACE which is 2. This means that for every character a-z. in our input, that character lives in 2d space and can be plotted with an x,y axis. When visualizing this, the groupings of these characters represent how closely the neural network thinks they're related.","When we increase the embedding space, we're essentially giving the neural network more dimensions to tune to gauge similarity between the input characters. So it can relate them amongst a variety of commonalities."]},{"l":"Visualization of embeddings"},{"l":"Code example"}],[{"i":"#","p":["calculating gradients across your neural network"]},{"l":"back propagation","p":["Back propagation is the process of taking a series of neurons, starting at the end, and calculating the effect each neuron has on the outcome of the network. We do this by calculating the gradient (see derivative) of each node. A popular use of back propagation is gradient descent."]},{"l":"Code","p":["Here's an example of an individual value node that would exist inside of a chain of nodes and the functions it needs for back propagation. You can think of a value node as a simplified neuron."]},{"l":"Code example"}],[{"i":"#","p":["take me back to calc 1"]},{"l":"Derivatives","p":["This all gets a bit fuzzy for me. I never took multi-variable calculus and neural networks seem to rely on them for partial derivatives... I think. But the concepts outlined below should be a good summary. I still have to check these out to stress test my knowledge here."]},{"l":"Basics","p":["The derivative of the function f(x) is the slope at point x, or how much the value changes at that point when increasing x by tiny amounts. In more explicit terms, it’s the value of:","For the function 3x^2 - 4x + 5 we can see that the following is true","So it’s increasing ever so slightly in the positive direction. To get the actual slope, we need to get the rise over run.","Which if we take the derivative of f(x) where x = 3 we get f'(x) = 6x - 4 = 6*3 - 4 = 14 which matches our equations.","This is important, because as we start making complex chains of nodes (a neural net), we want to know the derivative of the output wrt the individual nodes. That way we can tell how we have to change that node to manipulate the output of the function.","This is a good example, where we would want to know the derivative of L wrt c, so we know how to change c to impact L."]},{"l":"With respect to","p":["I struggled a bit to understand/remember what derivatives of a variable w.r.t. a function/another variable means.","You would say this is “The derivative of d(where d is the function) w.r.t. a. And what that means is whenever I change a, how does that change the output of d? Using the Constant Multiple Rule we know that the derivative of a variable times a function is that variable times the derivative of the function.","Why do we know that? Because some mathematician made it up. So in the above, k=b and f(x) = a which gives us f'(x) = a' = 1 which leaves us with b. So each time we increase a by 1, d increases by b. What happened to c? Since it's not one of the variables being operated on, it gets treated as a constant and the derivative of a constant is 0. That's a property of taking a partial derivative.","Something important to think about is that in d if we changed c to c^2 the outcome is still the same. Because w.r.t a, c is constant. Meaning changing a has no impact on c. However if the equation was something like","We would then end up with a derivative of"]},{"l":"Chain Rule","p":["The chain rule, in simple terms, says that if you have a composite function f(g(x)) and want to take the derivative of it, you can do f'(g(x)) * g'(x). I.e. the derivative of the composite function is the inner function within the derivative of the outer function, multiplied by the derivative of the inner function."]},{"l":"As it relates to back propagation","p":["More simply, as we navigate through back propagation, we will want to identify how one node affects the loss function. But this node could be one of many.","Using the above example, the local derivative of c \\space w.r.t \\space d is 1, because it’s addition. But how does c affect L? The Chain Rule gives us this. And what it says is, using the above variable names","Which in this case is -2 \\cdot 1 = -2. Why is \\frac{dd}{dc}=1? See addition. It's even nicer because as we get further along the back propagation we only need to multiply the local derivative by the gradient of the grandparent node. Take the following: If we want to do \\frac{dL}{da} we would only need to do \\frac{de}{da} \\cdot \\frac{dd}{de}, since it's all multiplication we don't have to chain it all the way back to L.","Another cool thing here is that since the back propagation only relies on multiplying the local derivative with the grandparent derivative, the calculation can be arbitrarily complex. It doesn't have to be + or * between nodes, it can be anything as long as we can derive it. This is helpful when doing back propagation on a neuron that has an activation function, so we can derive the value inclusive of the activation function."]},{"l":"Addition","p":["Doing the derivative of an additive operation w.r.t. a node is pretty easy. It equals 1. Why is that true? Take these nodes for example","Why did e become 0? Remember that's a property of taking a partial derivative wherein any variables we are not currently working with become constants. And the derivative of a constant is 0.","Also don't forget that 1 is only the local derivative, we have to apply the chain rule to get the gradient. Doing that we get 1 \\cdot -2 = -2."]},{"l":"References","p":["https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review"]}],[{"i":"#","p":["handling features for a model"]},{"l":"Encoding","p":["When feeding data into a neural network, we will need to represent out inputs (or features) in a more machine-friendly way. There are many ways to do this."]},{"l":"One-hot encoding","p":["This transforms our input set (e.g. all characters a-z) into an array that is 26-characters long. All values will be zero, except for the character we are feeding in, which will be a one. This is because the neural network doesn't give a shit about ascii or human-readable characters, it's purely operating on the underlying neurons and their inputs. See PyTorch for more info."]}],[{"i":"#","p":["fine tuning a neural network"]},{"l":"Gradient Descent","p":["Gradient descent is the process of fine tuning the weights and biases of neurons in a neural network to minimize our loss function."]},{"l":"Example","p":["We do this by performing back propagation across something like a multi-layer perceptron to calculate the gradients of each neuron. We do this so when we do a forward-pass through the MLP, we can compare the expected outputs against the actual outputs using a loss function. Gradient descent is then the process of adjusting the weights and biases of each neuron, to get our loss function as low as possible. The gradient of each neuron helps us understand whether to change the weights/biases of that neuron in a positive or negative direction to achieve the output we want.","Building off the multi-layer perceptron implementation, we can perform gradient descent with the following:","The reasoning for -0.1 here is actually super important. We have to remember the goal of this gradient descent is to lower the value of the loss function as much as possible. So when tuning our weights, we want to tune them such that they decrease the loss function. Luckily we know that the gradient will tell us how much that value will change the output. Let's look at some examples:","In this case, if we want to decrease the loss function, we want to decrease p.data, because p.grad tells us that for every n we increase p.data the loss function changes by n \\cdot 0.41. So it makes sense to instead do -0.1 * p.grad here.","But what if the signs are different?","In this case, increasing p.data decreases the loss function. If we do -0.1 \\cdot -0.41 we get 0.041 which will increase p.data and further decrease the loss function.","One more","If we increase p.data, that will lower the loss function. And just like the previous example -0.1 * -0.41 = 0.041 which will end up increasing p.data and lowering the resulting loss function. The sign of p.data actually has no effect here, it's only the sign of p.grad that matters. And we manage that by basically inverting it by multiplying with -0.1. If we were instead looking to maximize the loss function, we'd multiple by +0.1."]},{"l":"Code example"},{"l":"How gradients relate to the loss","p":["I got confused here for a bit trying to understand how we know that decreasing p.data would decrease the loss function. What if the output is too low, wouldn't we want to increase the data? It's important to remember the loss function is almost like a continuation of the neural network. You take the outputs from the network and calculate the loss functions with those. So the final item in the equation is actually the output of the loss function, not the output of the neural net. That means our gradients are now directly tied to the loss function, not the outputs of the NN, due to performing back propagation starting with the loss function.","This then confused me more, because if we have 4 forward passes of the NN resulting in a single loss, wouldn't back propagation update the weights/grads of the 4 individual forward passes, not the weights of the underlying model? While it may update the grad/weights for a lot of the intermediary calculations, all 4 of the forward passes used the exact same base neurons in their passes. So as we back propagate we sum the grads for each pass. However this does result in different weights of the neurons than if we ran 4 passes and back propagated individually. I'm still unclear on the tradeoffs here."]},{"l":"Zero grad","p":["You'll notice in the backwards pass above, we reset the gradient before each backwards pass. This is because after we change the weights and biases of a neuron, the gradient also changes and the previous values have no effect on it. So we reset all grads to zero so the next backwards pass can recalculate them from scratch."]},{"l":"Choosing a learning rate","p":["When figuring out the learning rate to multiply your gradient by, it's important not to have too large of a value as you could overstep the optimal point. But also too small of a value can cause learning to take forever.","As the number of iterations increases, it's common to use a strategy called \"Learning Rate Decay\" that lowers the learning rate as you get further into your training."]},{"l":"At scale","p":["When performing gradient descent at scale, it's common to only analyze a small subset of the total neurons to save on computation time. This means you'll choose a batch of neurons/layers, perform a forward pass on them, calculate the loss function on that batch, and update gradients within that batch accordingly."]}],[{"i":"#","p":["knobs to help you tune your neural network"]},{"l":"Hyper parameters","p":["Hyper parameters are the variables in your network that you get to control. Things like the embedding space of the inputs, the number of layers, the number of neurons in each layer, etc..."]},{"l":"Learning rate decay","p":["One of these parameters is the learning rate, which is how much you multiply the gradient by on each pass to tune the weights to the loss function. An easy way to decide on this rate is to exponentially increase the learning rate, visualize it, and identify where it starts to blow up. The below shows us that 10^{-1} seems to be our best fit."]}],[{"i":"#","p":["calculations to help you tune your neural network"]},{"l":"loss","p":["The loss is a single number that helps us understand the performance of the neural network. The loss function is how we calculate that number. A lot of the time in training a neural network is spent optimizing this loss function.","It's important to understand how the gradients of the neural network relate to the loss."]},{"l":"Mean-squared error loss","p":["You calculate this by subtracting the actual output of the neural network from the expected output, squaring them, and then taking the mean of all values you tested. I think this helps exaggerate values that are far from correct and shrink values that are closer to correct. But it also has the primary benefit of getting rid of the sign of the values, similar to abs.","The curious thing to me is that we don't actually take the mean of the summated squared losses, at least not in anything I've seen so far. So I'm hoping to figure that out. It seems like the division by N doesn't really matter, it's the squaring of the loss values that actually give us our metrics. Everything else is just syntactic sugar. This is a popular method when performing regression."]},{"l":"Example","p":["If we use our multi-layer perceptron we can provide it with our initial inputs xs and our expected outputs ys for 4 passes, feed those through the MLP, and then calculate the loss."]},{"l":"Binary cross-entropy loss","p":["TBD"]},{"l":"Max-margin loss","p":["TBD"]},{"l":"Regularization","p":["Not really familiar with this concept yet. But info is available here. It seems to be a way to try and smooth out irregularities in your model. One example is taking all of your neurons squaring them, taking the mean, and then multiplying that value by a small number like .01. You then add that to your loss function and it helps the loss function force your weights to be zero. More on that here."]},{"l":"Log likelihood","p":["When looking at probabilities, we will commonly see they're between 0 and 1, which makes it hard to work with them since multiplying them creates increasingly smaller numbers. To counter this, we use the log likelihood by taking the log of the probability. Giving us a better range of numbers."]},{"l":"Negative log likelihood","p":["This is the exact same except we invert the values. This is so that we can use it as a loss function and commonly with loss functions we optimize to get them as low as possible. But for a normal log likelihood the extremely negative values mean it's less likely. Inverting fixes that. It's also common to average the negative log likelihood, to give us a better estimation across the data set. This is a popular method when performing classification.","I came back to this trying to remember the purpose of the negative here. An important thing to remember is that when we calculate NLL, we do so on the softmax (or equivalent) of the logits. Which means all values are fractions (0,1](not sure if 0 should instead be inclusive here). Which are always negative when logged. So we invert them to get a positive number."]},{"l":"Cross-entropy loss","p":["This is another name for log loss likelihood. PyTorch has a really nice built-in method for this."]},{"l":"Maximum Likelihood Estimation","p":["This is when you train a model based purely on the statistical information you have access to, optimizing for likelihood of the various parameters."]},{"l":"Overfitting","p":["Overfitting is when you train the loss of your training set so much that it is too closely tied to that training set. Meaning the validation set will show much worse loss than the training set."]},{"l":"Initialization","p":["When first running your neural network, if you have an extremely high starting loss, it likely has to do with the weights and biases of your neurons. If they take up a wide range of numbers, that will result in a much greater variance in possible outcomes, increasing the loss function. If you instead start with biases set to 0 and weights multiplied by something like .1, you'll have a much smaller starting range. That more uniform distribution helps smooth out the loss.","I'm not sure how this applies to NNs with multiple layers and if you should apply this to the embeddings or not."]},{"l":"Estimating","p":["You can estimate what your starting loss should be by seeing what the loss would be for the uniform distribution of your data set. Assuming you have a character set of 27 characters, the expected loss would be:","You can also do this with a negative log softmax","Or cross_entropy"]}],[{"i":"#","p":["like the olympic torch, but nerdier"]},{"i":"pytorch","l":"\uD83E\uDD67\uD83D\uDD25 PyTorch","p":["If you read the word pytorch over and over, it really starts to lose it's meaning. Freaks me out when words do that. PyTorch is a production-grade machine learning framework written in Python that aids you in building and training neural networks.","Here's a simple tree showing the equation and back propagation we're doing, alongside the torch code to calculate the same.","You have to tell torch x1.requires_grad = True because they're leaf nodes and traditionally you don't want to calculate gradients for leaf nodes. My unconfirmed assumption, is because you usually aren't trying to change the inputs of the NN, you're trying to change the weights and biases of the neurons inside of it."]},{"l":"Code example"},{"l":"Tensors","p":["Torch and other frameworks use the concept of a Tensor. A Tensor is an n-dimensional array of scalar values. This is done to take advantage of computer parallelism to speed up calculations."]},{"l":"Tensor operations","p":["Tensors, i.e. matricies, have a lot of easy built-in mathematics provided by pytorch. For example, if you have a bi-gram model and want to get the probabilities of each character set in a specific row, you can do:"]},{"l":"Code example"},{"l":"Generators","p":["Generators in PyTorch allow you to enforce deterministic behavior by seeding the operations with a specific number. That way when they are run multiple times, they provide the same results."]},{"l":"Code example"},{"l":"Multinomial","p":["Allows you to extract values from a tensor based on the probabilities they represent. Given the tensor [.00, .99, .01] you would expect to get index 1 returned 99% of the time."]},{"l":"Code example"},{"l":"Broadcasting","p":["When working with tensors, you will be doing a lot of math and to save you time when doing matrix operations, PyTorch will automatically broadcast for you if the conditions are right. This means if you try to divide a 27x27 tensor by a 27x1 tensor, it'll automatically convert the latter into a 27x27 tensor where the 1 dimension is now copied across the other 26. This is easy to manually verify as well."]},{"l":"Code example"},{"l":"keepdim","p":["When using torch.sum there's a parameter called keepdim. What that does is makes sure that if you're summing across a set of rows, the output will have the dimension of those rows. Similarly, if you're summing across a bunch of columns, it will maintain that dimension. If you screw this up, when those values get broadcasted, they could get broadcasted in the wrong direction. E.g. broadcasting rows instead of the intended columns. In simpler terms, if you're using an n x m-dimensional array, and summing the n dimension, keepdim=True will output a 1 x m tensor with sums for each n."]},{"l":"Code example"},{"l":"One-hot encoding","p":["One-hot encoding can be used to transform your inputs into a format the neural network expects. PyTorch allows you to do this pretty easily."]},{"l":"Code example"},{"l":"Indexing tensors","p":["When trying to get indices out of a tensor you can actually provide a list to the array accessor to get multiple items returned.","You can also access multiple dimensions within a singular accessor"]},{"l":"Code example"},{"l":"Code example"},{"l":"Unbind","p":["There may be times where you want to take certain dimensions out of a tensor, you can do that using unbind","What unbind does it takes in an arbitrary tensor and returns all values from it for the particular dimension you provide. In the above example you can see when we pass in dimension 1 it returns the tensor with all column values joined together."]},{"l":"Code example"},{"l":"Cat","p":["torch.cat will concatenate tensors, it's purely a transformational operation, there isn't any multiplication/summation/etc... happening."]},{"l":"Code example"},{"l":"View","p":["Instead of needing unbind and cat, you can use the view function to transform the dimensions of a tensor.","And this is extremely performant because the way PyTorch stores tensors t.storage() is as a single vector, so it only requires transformation at the runtime, not in the storage."]},{"l":"Code example"},{"l":"Cross-entropy","p":["When calculating the negative log loss likelihood we commonly do something like:","PyTorch has this built-in with a simple cross_entropy function:","This function also handles edge cases far better. Because there is a limited range of numbers that computers can represent, if our logits contained 100, running .exp() on that results in inf. And adding inf to any math really fucks things up. The cross_entropy function handles this by subtracting the logits by the largest number in them. Now the max value is 0, but the distributions are all still equal so the probabilities aren't changed at all."]},{"l":"Code example"},{"l":"Code example"},{"l":"Visualize Activations","p":["You can visualize the activations of neurons in a specific layer with the following."]},{"l":"Code example"}],[{"i":"#","p":["treat your brain like an API"]},{"l":"Documentation as blog","p":["What's the point of a blog? I guess it depends on the person, but maybe it's to share your thoughts with the world, or to help learn by teaching others, or simply to use your creative spark. Whatever the purpose is, one thing that I dislike about blogs is they are one-dimensional. You have the post, maybe some links, a small summary of how you learned what you learned, and that's it. I want to see the ugly, the nitty gritty, all the nuggets of information that went into writing that blog. I want to see inside of the brain of the person that wrote it.","When stirring on this, it made me realize, how do companies organize their information? Is it a series of blog posts? No, it's documentation and wikis. It's structured, self-referencing content that builds off of itself. My hope here in the hallway is that anything I write will have its underlying information and my understandings of it outlined with complementary documentation. You can even check the commit history of this site to see how my learnings and misunderstandings of those topics evolved.","Finally, a blog is a static point-in-time representation of knowledge, which is annoying. My goal is that a lot of the underlying information will continue to evolve. Because of this, none of my posts have dates on them. If you are really interested, you can again check the commit history. Otherwise assume my knowledge is constantly changing and the underlying documentation should reflect that."]}]]