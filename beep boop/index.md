---
title: beep boop
layout: default
has_children: true
nav_order: 10
---

# ðŸ¤– 001110100

This is my working area for my current machine learning research. I'm starting back at the basics to try and rebuild my foundational knowledge. Most of this is my note taking while following Andrej Karpathy's [Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) course on Youtube. Fun fact: did you know he was [badmephisto](https://www.youtube.com/@badmephisto)?!?! I used his cubing tutorials a decade ago, insane.

## Splash pad

Random working thoughts go here.

Tensors are basically scalar values from [micrograd](https://github.com/karpathy/micrograd), but put into arrays to take advantage of parallelism in computing.

Loss functions help us identify the gap between what we expect from a function and what we actually got from it. The lower the loss, the more accurate the function is
